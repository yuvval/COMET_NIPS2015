
\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

%% COMET packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[square,numbers]{natbib}
\bibliographystyle{unsrtnat}
\usepackage[font={small}]{caption} % use small font for captions
%% For inserting 2 images in a row
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{floatrow}%

%%%%%%%%%%%%%%%%%%%%%%%%
%% packeges from ICML
% use Times
\usepackage{times}
\usepackage{graphicx} % more modern
\usepackage{subfigure} 
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
%\newcommand{\theHalgorithm}{\arabic{algorithm}}


%%%% COMET commands %%%%
\newcommand\todo[1]{\textbf{<ToDo:#1}!}
%\newcommand\mat[1]{\mathcal{#1}}
%\newcommand\mat[1]{\boldmath{#1}}
\newcommand\mat[1]{{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\newcommand{\T}{{}^\mathsf{T}}
\newcommand{\W}{\mat{W}}
\newcommand{\E}{\mat{E}}
\newcommand{\Hh}{\mat{H}}
\newcommand{\Pp}{\mat{P}}
\newcommand{\newW}{{\mat{W^{new}}}}
\newcommand{\eqdef}{\doteq}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tL}{\tilde{L}(\W)}
\newcommand{\frobsq}[1]{{\|#1\|_F^2}}
\newcommand{\frob}[1]{{\|#1\|_F}} 
\newcommand{\ignore}[1]{}

\newcommand{\q}{{\vec{q}}}
\newcommand{\p}{{\vec{p}}}
\newcommand{\trip}{{t}}
\newcommand{\qt}{{\q_{\trip}}}
\newcommand{\pt}{{\p_{\trip}}}
\newcommand{\triplet}{(\qt, \pt^{+}, \pt^{-})}


\newcommand{\A}{A}
\newcommand{\B}{\vec{b}}
\newcommand{\C}{c}
\newcommand{\invA}{A^{-1}}

\newcommand{\grd}{\frac{\partial \tL}{\W}}
\newcommand{\grdkl}{\frac{\partial \tL}{\W_{kl}}}


\newcommand{\uscalar}{{u}_{1}}
\newcommand{\uvec}{\vec{u}_{2:d}} 
\newcommand{\Wvec}{\W_{2:d,1}}
\newcommand{\Wscalar}{\W_{1,1}}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{apptheorem}{Theorem}
\newtheorem{applemma}{Lemma}

\renewcommand{\eqref}[1]{Eq.~(\ref{#1})}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\secref}[1]{Sec.~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}

%\DeclareMathOperator*{\argmin}{arg\,min}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Metric Learning One Feature at a Time}

\author{
Yuval Atzmon \\
The Gonda Brain Research Center
Bar Ilan University\\
52900, Israel\\
\texttt{yuval.atzmon@biu.ac.il} \\
\And
Uri Shalit \\
ICNC-ELSC & Computer Science Department, \\
The Hebrew University of Jerusalem, \\
91904 Jerusalem Israel,Affiliation \\
\texttt{uri.shalit@mail.huji.ac.il} \\
\AND
Gal Chechik \\
The Gonda Brain Research Center
Bar Ilan University\\
52900, Israel\\
\texttt{gal.chechik@biu.ac.il}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle
\vspace{-20pt}
\begin{abstract} 
Learning distance metrics from data amounts to optimization over the cone of positive definite (PD) matrices. Unfortunately, this optimization is difficult since projecting to the PD cone after update steps is prohibitively costly, as is restricting optimization to remain within the PD cone.

Here we describe COMET, a block-coordinate descent procedure, which efficiently keeps the search within the PD cone, avoiding both costly projections and unnecessary computation of full gradients. This is achieved by repeatedly optimizing a single row-and-column of the metric matrix, while using the Schur complement condition to guarantee that updates stay within the positive definite cone. 

As a block-coordinate descent procedure, COMET has fast convergence bounds showing linear convergence with high probability. When tested on benchmark datasets in a task of retrieving similar images and similar text documents, COMET significantly outperforms competing projection-free methods. Interestingly, COMET is naturally set up for learning metrics in face of a growing and changing feature set.
\end{abstract} 

\section{Introduction}
Metric learning, learning a measure of pairwise distance among data samples, is a fundamental task in machine learning. Learned metrics can be used to retrieve images that are similar to a query image, or recommend online content to a user visiting a webpage. It can also be used as a representation for supervised learning techniques based on distances, such as nearest-neighbors or kernel methods \cite{kulis2012survey}. 

Learning a metric is often cast as solving a convex optimization problem over the cone of positive definite (PD) matrices\ignore{ by optimizing a similarity measure $sim_W (x,y) = x^\top W y$ s.t. $W \in \R^{d \times d}$ is a PD matrix} \cite{kulis2012survey,bellet2013survey}. Unfortunately, obeying the PD constraint has a high computational cost. Projections to the PD cone require an eigendecomposition which is cubic in the number of features, and the alternative of restricting optimization within the PD cone is also hard to perform efficiently. As a result, finding efficient optimization algorithms for metric learning is an ongoing thorny challenge. 

This paper proposes a new efficient algorithm for learning PD metrics called  {\em{COordinate-descent METric learning}} (COMET), introducing a cost-effective method for performing block-coordinate descent over positive definite matrices.
COMET can efficiently optimize all the standard metric learning optimization loss functions, while maintaining a PD matrix model at all times during learning, with no need for eigendecompositions. COMET guarantees a linear convergence rate to the global optimum, and can leverage data sparsity for further computational gains. 

Our method works by updating the learned matrix one column and row at a time, thus updating the terms relating to one feature at each iteration. We use a $\log \det$ term as a barrier function for the PD cone, and employ the Schur complement to efficiently calculate the exact step size which will maintain the model within the PD cone. Evaluations of COMET on benchmark datasets show that it performs better than other scalable metric learning methods which avoid costly projections. 

Furthermore an important challenge for metric learning is the case where the set of features is not fixed in advance, but changes with time. This is a typical scenario in many real life applications of learning: as more data accumulates, it is possible to estimate more parameters accurately, so more features and signals are gradually added to existing systems. It is therefore desirable to develop algorithms that can learn metrics in face of a growing feature set. Our method naturally adapts to this setting, by optimizing the metric matrix one column-row at a time.



\section{Related work}
Learning distance metrics similarity measures from data has been intensively studied, see \citet{bellet2013survey, kulis2012survey} for recent surveys. We focus here on methods for learning linear Mahalanobis distance matrices as described below. A major challenge in this domain is to efficiently enforce the PD matrix constraint during optimization. The simplest is to project the learned matrix onto the cone of PSD matrices. This projection amounts to solving an eigendecomposition problem and is therefore costly (generally cubic in the feature dimensionality). In some cases, the number of projections can be cleverly reduced but each projection is still slow \cite{qianHD, qian}. A second common approach is to learn the parameters of a factored model $A^TA$. In this case, the learning problem is no longer convex. 

A second line of works avoid the costly projections by keeping optimization within the PSD cone. \citet{davis2007information} and \citet{lego} took this approach and introduced a logdet term as a log-barrier regularizer term. Another type of projection-free methods views a PSD matrix as a combination of other simpler PSD matrices. Very recently, \citet{hdsl} proposed to learn a PSD matrix as a weighted combination of rank-1 sparse PSD update matrices (HDSL). These matrices are all zeros except for $2\times2$ entries corresponding to pairs of feature. \citet{boost} introduced BoostMetric which learns the metric matrix using rank-1 (PSD) updates which are generated by solving a dual optimization problem in a boosting-based process. See also \citet{bi2011adaboost, liu2012robust}.

We take here an approach based on minimizing a strongly convex function using block-coordinate descent. There is a well established body of work analyzing the convergence of block-coordinate descent, eg. \cite{nesterov2012efficiency,richtarik2014iteration}. We discuss this further in Section 5 below.


% ===============================================
\section{The learning setup}
We address the problem of learning a metric over a set of
entities, like images or text documents based on their
relative pairwise similarities.

Formally, following \citet{OASIS}, let $\cal{P}$ be a set of entities $\{\p_1,...,\p_N\}$ each represented as a vector in $\Rd$. We measure the similarity of two samples $\q, \p \in \cal{P}$ using a bilinear
form parametrized by a model $\W \in \mathbb{R}^{d \times d}$, $S_{\W}(\q, \p) = \q\T \W \p$
When the matrix $\W$ is PSD, it can be factored as $\W = A^TA$ and used to define a distance measure over pairs of data points. This is achieved by using the root matrix $A$ to transform the data. The similarity $x^\top\W y$ between two data points $x$ and $y$ through the matrix $\W$, is equivalent to an Euclidean inner product  $(Ax)^\top(Ay)$ in the transformed space $x \mapsto Ax$. We assume that a weak form of supervision is given in the form of a ranking over triplets. Such ranking supervision is often easy to obtain and has been shown to achieve good performance \cite{weinberger2006dml,OASIS,qian}. We assume we have access to triplets of entities from $\cal{P}$, where each triplet $t$ consists of
a ``query'' instance $\qt \in \cal{P}$, and two instance $\pt^{+}, \pt^{-} \in \cal{P}$ such that $\qt$ is more similar to $\pt^{+}$
than to $\pt^{-}$.

We aim to find a similarity measure $S_{\W}$ that agrees with the ranking of these triplets, namely, $S_{\W}(\q, \p^{+}) > S_{\W}(\q,
\p^{-})$. To achieve this, we may use one of the following triplet loss functions
\begin{align}
\label{single-triplet-lossed}
l_{\W}^h(\qt, &\pt^{+}, \pt^{-}) = [1-\qt\T\W\pt^+ + \qt\T\W\pt^-]_{+}
 \\ \nonumber
 %\label{single-triplet-hinge-loss2}
l_{\W}^{hs}(\qt, &\pt^+, \pt^-) = [1-\qt\T\W\pt^+ + \qt\T\W\pt^-]_{+}^2
 \\ \nonumber
 %\label{single-triplet-log-loss} 
l_{\W}^{log}(\qt, &\pt^+, \pt^-) = log(1+exp(-\qt\T\W\pt^+ + \qt\T\W\pt^-)) \nonumber ,
\end{align}
where $[z]_{+} \eqdef \max(0,z)$. Given a batch of $\cal{T}$ triplets, and adding a Frobenius regularization term, we therefore wish to solve the following regularized optimization problem
\begin{eqnarray}
  \min_{\W}& \sum_{\trip=1}^{\cal{T}}  l_{\W}(\qt, \pt^+, \pt^-) + \frac{\beta}{2} \frobsq{\W}
 \\  \nonumber
   \rm{s.t.}& \W \succ 0 \quad,
\label{hingelt}
\end{eqnarray}
where $l_{\W}$ is any of the triplet loss functions, $\frob{\W}$ is the Frobenius norm of the matrix $\W$, $\beta$ is the regularization weight and $\W \succ 0$ refers to $\W$ being positive definite. This optimization problem is convex in $\W$ since the objective is a sum of convex functions in $\W$, and the domain $\W \succ 0$ is convex as well.

Previous metric learning approaches \cite{OASIS, qianHD, qian}, solved the constrained optimization problem by SGD or stochastic mini-batch gradient steps, while repeatedly projecting back to the convex cone of PD matrices. This projection amounts to solving an eigendecomposition problem and is therefore costly in runtime. An alternative approach is to use a log-barrier term and avoid projecting onto the PD cone \cite{davis2007information,lego}, yielding
\begin{equation}
\label{eq-logdet-loss}
L(W) = 
  \min_{\W} \sum_{\trip \in \cal{T}}  l_{\W}(\qt, \pt^+, \pt^-) - \alpha \log \det(\W) + \frac{\beta}{2} \frobsq{\W}
\end{equation}
where $\alpha$ is a hyper-parameter that determines the weight of the
$\log \det$ barrier term. Moreover, introducing the log-barrier term contributed to numerical stability when the optimum is close to the PSD cone and improved the empirical performance.

The loss in \eqref{eq-logdet-loss} can be minimized using gradient descent (GD), computing the gradient w.r.t. $\W$ for a set of triplets
\begin{equation}
  \frac{\partial {L (\W)}}{\partial \W} = \sum\limits_{t\in \cal{T}}{\{
  [\tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T]  }
  {l'}\triplet\} - \alpha \W^{-1} + \beta \W
  \label{gradMtx}
\end{equation}
%{l'}(\lambda_{t}^{\W})
%where $l'(x) \eqdef \frac{\partial {l(x)}}{\partial x}$, $\Delta\vec{p}_{t} \eqdef (\vec{p}_{t}^{-} - \vec{p}_{t}^{+})$
where $\Delta\p_t = \p_t^- - \p_t^+$, and $l'(x) \eqdef \frac{d{l(x)}}{dx}$ is the outer derivative of the loss function (see Appendix A in supplementary for derivation of the triplet loss). 


% ============================================================
\section{Coordinate-descent metric learning}

The learning setup above is commonly studied, but 
optimizing it using a gradient approach \eqref{gradMtx} raises two difficulties: It may be hard to computate and may distort the solution space.

First, the $\log \det$ regularizer yields $\W^{-1}$ in the gradient, so naive implementations of inverting the matrix are slow. Furthermore, matrix learning problems typically have a large number of parameters because the model scales quadratically with the feature dimension. A second problem concerns the effect of the $\log \det$ barrier on the solution space. In many applications the actual matrix that minimizes the above loss is near the boundary of the cone. This is the case when using human judgment on similarity as a supervision signal. In these cases, optimizing without using the PSD constraint - one often finds the optimum outside or near the boundary of the PSD cone. The problem is that if the only component keeping the solutions inside the PSD cone is the $\log \det$ term, it may distort the gradients near the boundary of the convex set. 

We propose an algorithm that alleviates these problems by using efficient block-coordinate descent that keeps optimization within the boundaries of the PSD cone.
Block-coordinate descent also enjoys provably fast convergence rates, and is especially useful when the block update can be performed efficiently, as we show below.

Our algorithm applies block-coordinate descent approach as follows. At each step, a single feature is drawn; the matrix entries on its row and columns are treated as a block and get updated. Importantly, we compute analytically a bound on the size of the update step, which guarantees that the updated matrix remains positive definite.

More formally, we perform the block updates as follows. First, since $\W$ is PD it is also symmetric, and we can replace $\W$ in \eqref{eq-logdet-loss} with $\tfrac{1}{2}(\W + \W\T)$. This guarantees that the gradient resulting from \eqref{gradMtx} is also symmetric. We draw a feature $k \in \{1 \ldots d$\} at random, and define a matrix $G$ that is all zeros except the values of $-\grd$ on the $i$-th row and $i$-th column. Finally we update the weight matrix using step size $\eta$:
\begin{equation}
    \newW = \W +\eta G.
\label{updateEq}
\end{equation}
\subsection{Selecting the step size $\eta$}\label{subsec:step}
Taking a coordinate step may take $\newW$ out of the PD cone. Fortunately, we can bound the step size to guarantee that $\newW$ remains PD using the Schur complement condition for positive definiteness. Without loss of generality, assume that the current round updates the first feature ($k = 1$). We write the pre- and post-update
matrices, as
\begin{equation}
  \W = \left[ \begin{matrix} \C & \B\T \\ \B & A \end{matrix} \right],
  \quad
  \newW = \left[ \begin{matrix} \C^* & \B^*\T \\ \B^* & A^* \end{matrix} \right],
  \label{schurNotationPreUpdate}
\end{equation}
 where $\C = \Wscalar \in \R$ (a scalar), $\B = \Wvec \in
\R^{d-1}$ (a column vector) and $A = \W_{2:d,2:d} \in \R^{(d-1)
\times (d-1)}$. Similarly for $A^*$, $\B^*$ and $\C^*$.

According to the Schur complement condition for positive definiteness
\citep[p. 650]{boyd2004convex}, $\newW$ is PD iff both
$A^*$ and $\C^* - \B^*\T A^{*-1} \B^*$ are positive definite.
Since $W \succ 0$ and $A$ is a minor of $\W$ which is left unchanged by the update, we have $A^* =
A \succ 0$. Moreover, $\C^* - \B^*\T A^{*-1} \B^*$ is a
scalar, yielding
\begin{equation}
  \newW \succ  0 \quad \Leftrightarrow \quad  \C^* - \B^*\T \invA \B^* >  0.
  \label{schurCond}
\end{equation}
Now let $\uscalar = \C^* - \C$ and $\uvec = \B^* - \B$ be the updated scalar and vector
obtained from $\eta G = \newW - \W$. We expand \eqref{schurCond} and
\eqref{gradMtx} (with $k=1$) yielding a necessary and sufficient condition for $\newW \succ 0$: $(\Wscalar + 2\eta \uscalar)-(\Wvec + \eta \uvec)\T \invA (\Wvec + \eta \uvec)   > 0$.
Grouping as a quadratic inequality in $\eta$, and using the notation from \eqref{schurNotationPreUpdate} we have
\begin{equation}
\label{PDUpdateCondQuadForm}
(\uvec\T \invA \uvec) \, \eta^2 
-2(\uscalar - \uvec\T \invA \B) \,\eta 
-(\C - \B\T  \invA \B) < 0 .
\end{equation}
For $\eta = 0$, this inequality always
holds since $\W \succ 0$ guarantees that $\C-\B^{\T} \invA \B >0$. As a result,
 \eqref{PDUpdateCondQuadForm} always has a real
root $\eta > 0$. This root provides an upper bound on $\eta$ that guarantees that $\newW$ is PD. The computational complexity of solving \eqref{PDUpdateCondQuadForm}, assuming $\invA$ is given, is $O(d^2)$ because computing the coefficients involves computing bilinear terms.
Furthermore, $(\newW)^{-1}$ and $\invA$ can be computed efficiently in $O(d^2)$ given $\W^{-1}$, using the Woodbury inverse matrix identity \cite{woodbury1950inverting}, see Appendix B in supplementary for details.

To conclude, we derived an upper limit for the step size of a block coordinate (row-column) step (\eqref{gradMtx} - \eqref{updateEq}) that guarantees  the updated matrix to be PD. The computational complexity for the evaluation
of \eqref{PDUpdateCondQuadForm} is $O(d^2)$, while keeping $O(d^2)$ matrix elements in the memory.
Our approach is summarized in Algorithm \ref{alg:comet}.
\begin{algorithm}[tb]
   \caption{COMET}
   \label{alg:comet}
\begin{algorithmic}[1]
   \STATE {\bfseries input:} training data, max number of steps, $\alpha$, $\beta$
   \STATE {\bfseries initialize:} 
   \STATE Generate a triplet set $\cal{T}$, Set  $\W  \leftarrow I_d$, $\W^{-1}  \leftarrow I_d$
   \REPEAT 
   \STATE Select a coordinate $k \in {1..d}$ uniformly at random.
   \STATE Compute $\invA$ using $\W^{-1}$; (see Appendix B).
   \STATE Compute the coordinate step gradient $G$; \eqref{gradMtx}.
   \STATE Select the step size $\eta$, with an upper limit from \eqref{PDUpdateCondQuadForm}.
   \STATE Update the metric to $\newW=\W+\eta G$.
   \STATE Update the metric inverse to $\newW^{-1}$; (see Appendix B).
   \UNTIL{stopping condition}
\end{algorithmic}
\end{algorithm}

\subsection{Analysis of computational complexity}

In this subsection we show that the asymptotic computational complexity of COMET is better than SGD based methods \cite{OASIS, qian} which require repeated projections. We also show that COMET has lower complexity than HDSL \cite{hdsl}, and finally we show that in the case of many samples with at least moderate sparsity, COMET's computational complexity is better than LEGO \cite{lego}.

We first evaluate the computational complexity of a single coordinate step \eqref{gradMtx}, including the computation of the gradient and updating of $\W$, $\W^{-1}$ and $\invA$. Consider first the computation of the gradient. For the hinge-loss case $l^{h}_W$, each element $\delta_{i,j}$ of the gradient matrix \eqref{gradMtx} equals
\begin{equation}
    \delta_{(i,j)} = \sum\limits_{t\in \cal{T}}{ [\tfrac{1}{2}[(\vec{q}_{t})_i(\Delta\vec{p}_{t}\T)_j + (\Delta\vec{p}_{t}\T)_i(\vec{q}_{t})_j\T] } \cdot \textbf{1}(\lambda_{W}^t) - \alpha \cdot \W^{-1}_{i,j} + \beta \cdot \W_{i,j},
\label{gradMatElem}
\end{equation}
where $\lambda_{W}^t \eqdef 1+\qt\T \W \Delta\p_{t}$ is the linear part a triplet loss

For dense data, evaluating the sum over triplets costs $O(T)$ operations, where $T$ is the number of triplets. However, for sparse data with a sparsity coefficient $ 0< \gamma <1 $, evaluating the sum in \eqref{gradMatElem} costs an average of $O(\gamma^2 T)$ operations, because we can accumulate only the elements that are both non-zeros in $(\vec{q}_{t})_i$ and in $(\Delta\vec{p}_{t}\T)_j  $ and likewise for $(\vec{q}_{t})_j$ and $(\Delta\vec{p}_{t}\T)_i$.   To efficiently evaluate the indicator functions $\{ \textbf{1}(\lambda_{W}^t) \}_{t \in T}$ on \eqref{gradMatElem}, we keep an array of the linear terms $\{\lambda_{W}^t\}_{t \in T}$. Computing all the gradient elements $\delta_{(k,1:d)}$ in a single row $k$ costs $O(d\cdot \gamma^2 T)$.
Maintaining and updating $\W^{-1}$ and $\invA$, and computing the optimal step size following equations on Appendix B (in supplementary), each costs $O(d^2)$ operations. 
To conclude, the total computational complexity per block-coordinate step is $O(\gamma^2 d T + d^2)$. When taking $Nd$ coordinate steps, the overall complexity of COMET is 
\begin{equation}
O(N \cdot (\gamma d)^2 T + N \cdot d^3)
\label{cometComplexity}
\end{equation}
We found empirically that COMET converges within $N= 5 - 10$. As a comparison, consider using SGD or mini-batches for the objective of \eqref{hingelt} and projecting onto the PD cone every $P$ triplets ($P \ll T$), as proposed in \cite{OASIS,qian}. The computational complexity per data pass becomes $O((\gamma d)^2 T + \frac{T}{P} d^3)$. This approach is slower than COMET and only reaches the complexity of COMET when projections are very rare. For example, \citet{qian} used mini-batches of $P=10$ triplets. With a total of $T=100k$ triplets, yielding a computational complexity ~1000 times larger than COMET.

Compare further with \citet{hdsl}. COMET’s complexity is better than HDSL. The fast heuristic version of HDSL is $O(Md+Tk)$ per coordinate step, where $M$ is the size of mini-batch, $k$ is the iteration number. This is summed over $k=1,...,O(d^2)$ iterations, since each HDSL step considers a single pair of features, updating 4 matrix entries as opposed to $2d-1$ entries in COMET. Overall, this yields $O(Md^3+Td^4)$ computations for HDSL, compared with \eqref{cometComplexity} of COMET. Since both $M$ and $N$ are typically small, this means HDSL is more costly than COMET by a factor of $\frac{d^2}{N}$. In our experiments, HDSL sometimes uses much less than $O(d^2)$ iterations, but then achieves a significantly inferior test error.

Finally, we compare with the complexity of LEGO \cite{lego}. LEGO requires $O(d^2)$ computation per constraint. Thus for $N$ passes over $T$ triplets LEGO's complexity is $O(N\cdot d^2 \cdot T)$. Assuming an equal number of passes over the triplet, we find that COMET's complexity \eqref{cometComplexity} is asymptotically better than LEGO as long as $N \cdot T \cdot (1-\gamma^2) > d^2$. That is, whenever the data is even moderately sparse, and the number of triplets is larger than the number of matrix parameters.

{\bf COMET Memory footprint}: Keeping the data triplets in memory takes $O(\gamma d T)$ elements and holding $\W$ and $\W^{-1}$ costs $O(d^2)$. The total memory usage is $O(\gamma d T + d^2)$. 

\section{Convergence rate}
Our method is based on minimizing a strongly convex function using block-coordinate descent. There is a well established body of work showing that with non-overlapping blocks, block-coordinate descent iterates converge w.h.p. in a linear rate to the optimum value \cite{nesterov2012efficiency,richtarik2014iteration}.
However, the blocks we use in our method are overlapping - for example the $(1,2)$ coordinate of the matrix is a part of both the 1\textsuperscript{st} and the 2\textsuperscript{nd} column-row. To address this case, we use a more general convergence result applicable to overlapping blocks, given by \citet{richtarik2013optimal}. \citeauthor{richtarik2013optimal} give a very general result, suitable for \emph{any} distribution over the set of coordinate subsets. 
Specifically of interest to us, \citeauthor{richtarik2013optimal} give sufficient conditions for a linear convergence rate of overlapping block-coordinate descent with a strongly convex smooth objective. 
We use a relatively simple distribution over coordinate subsets: we have $d$ overlapping blocks corresponding to the column-rows of the matrix, each sampled with a probability $p_i$, $i=1 \ldots d$ (in the experiments below we used a uniform $p_i = \frac{1}{d}$).

The step sizes implied by the convergence theory presented in this section are conservative underestimates, especially since many of the constants involved in obtaining the step-sizes cannot be evaluated exactly but can only be upper-bounded. In practice, we found that much faster convergence is gained using larger steps while staying within the PD cone, using the Schur complement driven procedure described in detail in section \ref{subsec:step}.

To show convergence, we must prove our objective satisfies two assumptions: Assumption 1, called ``Expected Separable Overapproximation'', is that in expectation over the choice of blocks the function is smooth w.r.t. an inner product given by the coordinate probabilities. Assumption 2 is that the objective is strongly convex. In addition, for technical reasons the objective must be differentiable. This means that technically our proof is only valid for the squared hinge-loss and log-loss, but not the non-differentiable hinge-loss.

To fulfill the conditions in \cite{richtarik2013optimal}, we must slightly modify the objective function $L({\W})$. The objective $L(\W)$ is strongly convex but is not smooth, since the gradient of the $\log \det$ term is unbounded near the envelope of the positive definite cone. Let $\tilde{L}({\W}) = L({\W + \kappa I_d})$ be a modified version of the loss in Eq. \ref{eq-logdet-loss}, where $I_d$ is the $d \times d$ identity matrix, and $\kappa>0$ is a fixed parameter.
Note that our algorithm can easily minimize $\tilde{L}$, the only difference being that we now need to maintain and update both $\W^{-1}$ and $(\W+\kappa I_d)^{-1}$, which does not change the asymptotic computational complexity. The additional $\kappa I_d$ term acts as a bias term, where we add a constant Euclidean distance term to the distance we learn, but in practice we found that simply setting $\kappa=0$ had no detrimental effect on our performance. 

We show that the modified objective $\tilde{L}$ obeys Assumptions $1$ and $2$ of \citet{richtarik2013optimal}. Thereby, according to Theorem 3 of \citeauthor{richtarik2013optimal}, Algorithm \ref{alg:comet} converges w.h.p. to the optimum value in a linear rate:

\begin{theorem}
Let $\W^t$ be the $t$-th iterate of Algorithm \ref{alg:comet} with objective function $\tL$, sampling each column-row $i$ with probability $p_i$. Let $\tilde{L}^*$ be the optimal value of $\tL$ on the PSD cone. Let $\beta^* \geq \beta$ be the strong convexity parameter of $\tL$, $M^1$ a constant depending on the norm of the dataset, $\Lambda = \max_i \frac{1}{p_i}$, $\rho >0, \epsilon>0$.

If $t > \frac{\Lambda (M^1 + \alpha d (1/\kappa)^2 + \beta)}{\beta^*} log \left( \frac{\tilde{L}(W^0) - \tilde{L}^*}{\epsilon \rho}\right)$ then: $Prob(\tilde{L}(\W^k) - \tilde{L}^* \leq \epsilon) \geq 1-\rho$.
\end{theorem}
See Appendix C (in supplementary) for the proofs relating to the statements in this section.


\begin{figure}[h]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,top},capbesidewidth=4cm}}]{figure}[\FBwidth]
{\caption{\textit{Precision-at-top-k} of COMET, evaluated on the test set, as a function of the number of coordinate steps, normalized to the data dimension. Error bars denote the standard error of the mean across 5 random train/test partitions (80\%/20\%)}\label{cometConvergeFig}}
{\includegraphics[width=9cm]{COMET_convergence_NIPS}}
\end{figure}

\section{Experiments}
We evaluate COMET on two datasets and compare its performance with four metric-learning approaches. All the approaches we compare with learn a Mahalanobis metric matrix and avoid repeated projections to the PD cone. 

\subsection{Competing approaches}

\textbf{COMET}. The algorithm described in Algorithm~\ref{alg:comet}. We experimented with the linear hinge-loss for the triplets loss, see \eqref{eq-logdet-loss}.

\textbf{Euclidean}. The Euclidean distance in the original feature space. COMET is initialized using the identity matrix, which is equivalent to this distance measure.

\textbf{HDSL} \cite{hdsl}. An approach tuned for high-dimensional sparse data. HDSL learns a convex combination of rank-1 PSD matrices that are all zeros except for a $2\times2$ pair of features elements. It iteratively adds these matrices, one feature-pair at a time, to control the number of active features.

\textbf{LEGO} \cite{lego}. This approach uses a $\log \det$ barrier term to enforce the PD constraint. The main variant of LEGO aims to fit pairwise distances. We used a variant of LEGO that, like COMET, learns from relative similarities. Loss is incurred for same-class samples that are farther than a certain distance, and different-class samples closer than a certain distance.

\textbf{BoostMetric} \cite{boost}. Based on the observation that any positive semidefinite matrix can be decomposed into linear positive combination of rank-1 matrices, BoostMetric uses rank-1 PSD matrices as weak learners within a boosting based learning process.
\begin{figure}[h]
\floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,top},capbesidewidth=4cm}}]{figure}[\FBwidth]
{\caption{(best seen in color) Comparison of the performance of COMET, LEGO, BoostMetric, HDSL and the Euclidean metrics across the datasets. Each curve shows the \textit{precision-at-top-k}, evaluated on the test set, as a function of $k$ neighbours. The results are averaged across 5 train/test random partitions (80\%/20\%), error bars are standard error of the means.}\label{precFig}}
{\includegraphics[width=9cm]{Precision_at_K_all_datasets_NIPS}}
\end{figure}

\subsection{Datasets}
We evaluate COMET on three benchmark datasets.

\textbf{Reuters CV1} is a widely used collection of English text documents. We used the 4-class subset of Reuters CV1 introduced in \cite{CaiRCV14} that was recently tested for metric learning in \cite{hdsl}. We used the \textit{infogain} criterion \cite{infogain} to select subsets of 5000 and 1000 features. This is a discriminative criterion, which measures the number of bits gained for category prediction by knowing the presence or absence of a term in a document. Each document was represented as a bag of words, where the weights of the selected features were normalized using \textit{tf-idf}. The sparsity of this dataset, after selecting the 5000 infogain features, is 1.3\%. We used 100,000 triplets (and 200,000 LEGO constraints) for training on Reuters CV1. This is the same scale that was used in \cite{hdsl}. For training HDSL, we took 8000 iterations as in \cite{hdsl}. BoostMetric could not converge on this dataset due to memory and runtime issues caused by the large number of features.

\textbf{Caltech256} is a dataset of labeled images used for visual object recognition. We used the subsets of 50 and 249 classes tested for metric learning in \cite{OASIS}. This set contains 65 images per class (total of 3250 images), represented with ~1000 \textit{bag-of-local-descriptors} features provided by these authors. The sparsity of this dataset is 3.3\%. We used 135,000 triplets (and 300,000 LEGO constraints) for training, roughly as was used in \cite{OASIS}. To select the number of iterations in training HDSL we used early stopping on a validation set. BoostMetric was slow on this dataset, and used a large amount of memory. For a fair comparison, we took the number of COMET coordinate steps to be the maximal number of BoostMetric rank-1 updates.

\textbf{Protein} is a LIBSVM \cite{libsvm} dataset with 3 classes, 357 features and 24387 samples. It was recently tested for metric learning in \cite{qian}. We used 20,000 triplets (and 50,000 LEGO constraints) for training.

\subsection{Experimental setup and Evaluation measures}
In both datasets, two samples are considered similar if they share the same class label. Each data set is tested on a two layer 5 fold cross validation experiment. We use the same (frozen) random splits across all approaches. Training all learners with the exact same set of triplets, except for LEGO that uses pairs constraints. We verified that we choose the triplets/constraints number in a regime such that test performance converges (figures not shown due to space constraints). We generated triplets randomly while keeping a fixed number of triplets per query sample.

We evaluated the performance of all algorithms using standard ranking precision measures based on nearest neighbors. For each query instance in the test set, all other test instances were ranked according to their similarity to the query instance. The number of same-class instances
among the top k instances (the k nearest neighbors) was computed. When averaged across test
instances, this yields a measure known as \textit{precision-at-top-k},
providing a precision curve as a function of the rank $k$. \textit{precision-at-top-k} for k >1 is useful in retrieval of multiple objects that are similar to a query object, e.g. an image search engine.
\subsection{Results}
We evaluated the \textit{precision-at-top-k} on the test set, as a function of $k$ neighbours and averaged the results across 5 random train/test partitions (80\%/20\%)\footnote{Results for RCV1 1K feat., Caltech 249 categories and Protein are in the supplemental section.}.
Figure \ref{cometConvergeFig} shows the \textit{precision-at-top-k} over the test sets as it progresses during learning. We observe that convergence is usually achieved after $6\times d$ to $8 \times d$ coordinate steps.
Figure \ref{precFig} compares the precision obtained with COMET with four competing approaches, as described above. COMET achieved consistently superior or equal results throughout the full range of number of neighbours tested. 

Surprisingly, when studying the optimal values of hyper parameters, we found that the Frobenius regularizer obtained very small weights. Setting its coefficient to zero did not harm the performance. We also compared COMET with OASIS, a method that learns a non-PSD similarity matrix, which has been shown to yield better precision on the Caltech data \cite{OASIS}. Interestingly, COMET achieves near identical precision to OASIS. 

\tabref{runtimes}, compares the run times of the different approaches\footnote{We had partial results with Boostmetric, hence, we omitted its results from the run time table.}. LEGO was fastest; HDSL (with core implemented in C) is sometimes faster and sometimes slower than COMET (implemented in MATLAB). Importantly, COMET converged to a better optimum.

\begin{table*}[t]
\caption{Run time statistics, minutes.}
\label{runtimes}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\hline
Dataset     & COMET           & LEGO            & HDSL             \\ 
\hline
Reuters CV1, (5K feat.)&  1656 $\pm$    13 &   423 $\pm$    29 &   522 $\pm$    24  \\ 
Caltech256 50 Cat. (1K feat.)  &    72 $\pm$     2 &    15 $\pm$     3 &   495 $\pm$    73  \\ 
Caltech256 249 Cat. (1K feat.) &   220 $\pm$    10 &    20 $\pm$     3 &        *        \\
Reuters CV1 (1K feat.) &   157 $\pm$     3 &    11 $\pm$     3 &   115 $\pm$    18  \\ 
protein, (357 feat.)  &    80 $\pm$    10 &     1 $\pm$     0 &   163 $\pm$    11 \\ 
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\section{Summary and future directions}
We presented an approach for learning a distance metric from data samples, continuously restricting the solutions to the positive cone, but avoiding runtime-costly projections. The approach is based on block-coordinate-descent and iteratively updates a single row-column in the matrix that corresponds to a single feature. Our experiments have demonstrated that its \textit{precision-at-top-k} significantly outperforms all competing PSD methods we compared with, while maintaining lower computational complexity. It even achieves a comparable \textit{precision-at-top-k} as \cite{OASIS}, even though \cite{OASIS} is not limited to PSD matrices and only applies a single projection post training

An important challenge for learning is the case where the set of features is not fixed in advance, but changes with time. This is a typical scenario in many real life applications of learning: as more data accumulates, it is possible to estimate more parameters accurately, so more features and signals are gradually added to existing systems. It is therefore desirable to develop algorithms that can learn metrics in face of a growing feature set. COMET can be effortlessly adapted to this setup, by extending the existing learned matrices and sample more heavily rows and columns that correspond to new features. The specifics of this approach are the topic of further research.


\newpage

\small{
\bibliography{comet}
%\bibliographystyle{icml2015}
}



\end{document}
