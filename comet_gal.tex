%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% COMET ICML 2015 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

%% COMET packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}

%%%%%%%%%%%%%%%%%%%%%%%%


% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For theorems and proofs
\usepackage{amsthm}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Metric learning using Block Coordinate Descent}

\begin{document} 

\twocolumn[
\icmltitle{Metric Learning using Block Coordinate Descent}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{metric learning, similarity learning}

\vskip 0.3in
]

%%%% COMET commands %%%%
\newcommand\todo[1]{\textbf{<ToDo:#1}!}
%\newcommand\mat[1]{\mathcal{#1}}
%\newcommand\mat[1]{\boldmath{#1}}
\newcommand\mat[1]{{#1}}
\renewcommand\vec[1]{\mathbf{#1}}
\newcommand{\T}{{}^\mathsf{T}}
\newcommand{\W}{\mat{W}}
\newcommand{\E}{\mat{E}}
\newcommand{\newW}{{\mat{W^*}}}
\newcommand{\eqdef}{\doteq}
\newcommand{\Rd}{\mathbb{R}^d}
\newcommand{\tL}{\tilde{L}(\W)}

\newcommand{\frob}[1]{{\cdot \|#1\|_F^2}} 

\newcommand{\ignore}[1]{}

\newcommand{\q}{{\vec{q}}}
\newcommand{\p}{{\vec{p}}}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}


%\DeclareMathOperator*{\argmin}{arg\,min}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract} 
TBD
\end{abstract} 

\section{Introduction}
Learning a measure of pair-wise similarity or distance among data
samples is a fundamental task in machine learning. Learned metrics can
be used to retrieve a similar image or document given an example
\cite{}, or be used as a representation for a supervised learning
technique that is based on distances, like nearest-neighbors or kernel
methods \cite{}.


\section{Related work}
There has been a lot of work on learning similarity measures and
distance metrics from data, see \cite{} for a review. Specifically,
several approaches have been proposed for learning a positive definite
Mahalanobis matrix.

{XXX YUVAL XXX}


\section{The learning setup}
We address the problem of learning a distance metric over a set of
entities, like images or text documents based on data on their
relative similarities. 

Formally, following \cite{OASIS}, let \cal{P} be a set of entities
$\{\p_1,...,\p_N\}$ each represented as a vector in $\Rd$.  We measure
the similarity of two samples $\q, \p \in \cal{P}$ using a bilinear
form parametrized by a model $\W \in \mathbb{R}^{d \times d}$.
\begin{equation}
  S_{\W}(\q, \p) = \q\T \W \p
\end{equation}
\newline
{\bf{XXX Yuval, you need to explain how the similarity is later used a sa
distance measure (since W is positive, one can project using sqrtm(W)
and use euclidean distance in the projected space  XXX }}
\newline

We assume that a weak form of supervision is given in the form of
ranking relation over triplets. This form of supervision is often easy
to obtain and has been shown to achieve good performance
\cite{lmnn,oasis,qian}. Specifically, we assume we have access to
triplets of entities from $\cal{P}$, where each triplet $t$ consists of
a ``query'' instance $\q_{t} \in \cal{P}$, and two instance $\p_{t}^{+},
\p_{t}^{-} \in \cal{P}$ such that $\q_t$ is more similar to $\p_t^+$
than to $\p_t^-$.

We aim to find a similarity measure $S_{\W}$ that agrees with the
ranking of these triplets, namely, $S_{\W}(\q, \p^+) > S_{\W}(\q,
\p^-)$. To achieve this, we define a hinge loss for a triplet
\begin{equation}
  l_{\W}(\q_t, \p_t^{+}, \p_t^{-})=[1-\q_t\T \W \p_t^{+}+\q_t\T\W\p_t^{-}]_{+}
  \label{hingelt}
\end{equation}

where $[z]_{+} \eqdef max(0,z)$.  Given a batch of $T$ triplets, and
adding a Frobenius regularization term, we therefore wish to solve the
following regularized optimization problem
\begin{eqnarray}
  \min_{\W}& \sum_{t=1}^T  l_{\W}(\q_t, \p_t^{+}, \p_t^{-}) + \beta \frob{\W}
 \\  \nonumber
   \rm{s.t.}& \W \succ 0
  \label{hingelt}
\end{eqnarray}
where $\frob{\W}$ is the Frobenius norm of the matrix $\W$, $\beta$ is
the regularization weight and $\W \succ 0$ refers to $\W$ being
positive definite. This optimization problem is convex in $\W$ since
the objective is a sum of convex functions in $\W$, and optimization
is over the cone of positive definite matrices which is convex.

In several previous metric learning approaches \cite{pola, others}, the
constrained optimization problem is solved in an online way by making
stochastic gradient steps, while repeatedly projecting back to the
convex cone of positive definite matrices. This projection amounts to
solving an eigen-decomposition problem and is therefore costly in
run-time. To avoid this costly projection, we add a log-barrier term,
as in \cite{itml}
\begin{equation}
  \min_{\W}& \sum_{t=1}^T  l_{\W}(\q_t, \p_t^{+}, \p_t^{-})+  \alpha  \log \det(\W) +  \beta \frob{\W}
  \label{eq-logdet-loss}
\end{equation}
where $\alpha$ is a hyper-parameter that determines the weight of the
logdet regularizer, and the $\log \det$ term ensures that only
positive definite matrices are obtained when minimizing
\ref{eq-logdet-loss}.





\ignore{ An outline of our next steps is that we will define an
objective function of the total hinge loss over all the triplets, a
matrix Frobenius norm regularization and a $\log \det$ regularization
over $\W$ that assigns a high cost when $\W$ reaches close to the PD
cone boundaries.  We will take a block coordinate descent approach
which ensures that each step is taken within the PD cone. Each
coordinate step is taken over a single row and column $k$ of $\W$. An
upper limit on the step size is evaluated using the Schur Complement
condition for positive definiteness \todo{Add citation} and we take a
step with a step size within that boundary. }

\ignore{
\subsection{The objective}
We define an objective function
\begin{equation}
  L(\W)=\sum\limits_{t\in T}{l_{\W}(\q_t, \p_{t}^{+}, \p_{t}^{-})} -
  \alpha \cdot \log \det(\W) + \tfrac{1}{2} \beta \cdot \| \W
  \|_{F}^{2}
\end{equation}
where $T$ denotes the batch of triplets, $\alpha, \beta$ are hyper
parameters, $\log \det(\W) \eqdef \log (\det(\W))$ and $\| \W
\|_{F}^{2}$ is the Frobenius norm of $\W$. It is comprised of the
total hinge loss over all the triplets and a $\log \det$
regularization over $\W$ that assigns a finite high cost when $\W$ is
PD and is close to the PD cone boundaries.  Our objective is to find
$\W$ that minimizes $L(\W)$, such that $\W$ is PD ($\W\succ 0$).

We observe that the PD cone is a convex set and that the objective
function is a linear sum of convex functions over the PD cone. Hence
the optimization over the objective is a convex problem with a single
minima which is global.
}


\section{COMET: Coordinate-descent metric learning}
The optimization problem of \ref{} can be solved using stochastic
gradient approach similar to \cite{itml}. The gradient of the loss
w.r.t. $\W$ is
\begin{multline}
  \frac{\partial {L (\W)}}{\partial \W} = \sum\limits_{t\in T}{\{
  [\tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T \} } \cdot
  \textbf{1}(l_t(\W))] \\- \alpha \cdot \W^{-1} + \beta \cdot \W
  \label{gradMat}
\end{multline}
where $\Delta\p_t = \p_t^+ - \p_t^-$, and ${\textbf{1}(l_t(\W))$ is an
indicator function which obtains a value of one if the loss is
positive, and zero otherwise (see appendix for derivation details).


Here however, we take a different approach and use block-coordinate
descent in a way that makes it easy to keep the search over $\W$
within the boundaries of the positive definite cone.

Coordinate descent is {\bf{BLA BLA}} explain some potential benefit
\cite{}. ...


We take a block-coordinate-descent approach. At each step, a single
feature is randomly drawn, and all the matrix entries on its row and
columns are treated as a block, and being updated. Importantly, we 
compute analytically a bound on the size of the update step, which
guarantees that the updated matrix remains positive definite.

More formally, let $k$ be the index of a randomly drawn feature, and
$\mat{G}$ be the gradient from \ref{}, computed only over the matrix
entries $\W_{1k}, \ldots, \W{dk}$ and $\W_{k1}, \ldots, \W{kd}$ (a
total of $2d-1$ entries). The update step is 
\begin{equation}
   \newW = \W +\eta\cdot\mat{G}
   \label{updateEq}
\end{equation}

{\ignore{
Since PD matrices are symmetric, we require the update $\mat{G}$ of \eqref{updateEq} to be symmetric:
\begin{equation}
  \mat{G} = \vec{u}\cdot\vec{e_k}\T + \vec{e_k}\cdot\vec{u}\T
  \label{gradMtx}
\end{equation}
where $\vec{u}$ is a column vector that equals the column $k$ of the
(symmetric) gradient matrix of the objective \eqref{gradMat},
$\vec{e_k}$ equals an elementary vector for selecting a column $k$ of
a matrix.

[XXX YUVAL, WHy IS THIS NECESSARY? FOr NUMERICAL STABILITY? XXX}

}} 

\subsubsection{choosing the step size $\eta$}
We now show how the step size can be bounded to guarantee tat $\newW$ is PD.

GAL STOPEED HERE

Following, using the Schur complement condition for positive
definiteness \todo{AddREF!}, we derive an upper limit for the step
size. We will show that taking a step size on $\mat{G}$ which is
smaller than the upper limit guarantees that the update
\eqref{updateEq} will keep $\newW$ inside the PD cone.

The pre-update matrix $\W$ can be formulated according to the Schur complement notation as: 
\begin{equation}
\W= \left[ \begin{matrix} C & \vec{B\T} \\ \vec{B} & \mat{A} \end{matrix} \right]
\label{schurNotationPreUpdate}
\end{equation}
where $C = \W_{(1,1)}$ (scalar), $\vec{B} = \W_{(2:,1)}$, $\mat{A} = \W_{(2:,2:)}$

We apply the same notation to $\newW$
\begin{equation}
\mat{\newW}= \left[ \begin{matrix} C^* & \vec{B^*}\T \\ \vec{B^*} & \mat{A^*} \end{matrix} \right]
\end{equation}
According to the Schur complement condition for positive definiteness \todo{AddREF!}, $\newW$ is PD iff $\mat{A}^*$ and $C^* - \vec{B}^*\T \mat{A}^{*-1} \vec{B}^*$ are both positive definite:
\begin{equation}
\newW \succ  0 \Leftrightarrow (\mat{A}^* \succ  0, C^* - \vec{B}^*\T \mat{A}^{*-1} \vec{B}^* \succ  0)
\label{schurCondPreliminary}
\end{equation}
We note that $\mat{A}^* = \mat{A}$ because we only update in this step
the row-column $k=1$. $\mat{A}$ is a minor of $\W \succ 0$. Resulting
with $\mat{A}^*=\mat{A} \succ 0$. Moreover, $C^* - B^*\T A^{*-1} B^*$
is a scalar. Therefore \eqref{schurCondPreliminary} reduces to:
\begin{equation}
\mat{W^*} \succ  0 \Leftrightarrow (C^* - B^*\T A^{-1} B^* >  0)
\label{schurCond}
\end{equation}
Next, we expand \eqref{schurCond} according to \eqref{updateEq} and \eqref{gradMtx} (with $k=1$) yielding a condition for $\newW \succ  0$
\begin{equation}
\begin{array}{ll} 
(\W_{(1,1)} + 2\eta \vec{u}_{(1)})  \\
-(\W_{(2:,1)} + \eta \vec{u}_{(2:)})\T \mat{A}^{-1} (\W_{(2:,1)} + \eta \vec{u}_{(2:)})  & > 0
\end{array}
\label{PDUpdateCondNonSimpl}
\end{equation}
Grouping \eqref{PDUpdateCondNonSimpl} as a quadratic inequality, we get:
\begin{equation}
\begin{array}{ll} 
-(\vec{u}_{(2:)}\T \mat{A}^{-1} \vec{u}_{(2:)})\eta^2 \\
+2(\vec{u}_{(1)} - \vec{u}_{(2:)}\T \mat{A}^{-1}\W_{(2:,1)})\eta\\
+(\W_{(1,1)} - \W_{(2:,1)}\T  \mat{A}^{-1} \W_{(2:,1)}) & > 0
\end{array}
\label{PDUpdateCondQuadFormWithW}
\end{equation}
or according to \eqref{schurNotationPreUpdate}:
\begin{equation}
\begin{array}{ll} 
(\vec{u}_{(2:)}\T \mat{A}^{-1} \vec{u}_{(2:)}) \eta^2 \\
-2(\vec{u}_{(1)} - \vec{u}_{(2:)}\T \mat{A}^{-1}\vec{B})\eta \\
-(C - \vec{B}\T  \mat{A}^{-1} \vec{B}) & < 0
\end{array}
\label{PDUpdateCondQuadForm}
\end{equation}
We note that the condition \eqref{PDUpdateCondQuadForm} always holds
for $\eta = 0$ because $\W\succ0$ and therefore the RHS of
\eqref{schurCond} is true (after replacing $\W^*$ by $\W$). Hence, it
ensures that solving \eqref{PDUpdateCondQuadForm} according to $\eta$
results with an upper limit for $\eta$.  The computational complexity
of \eqref{PDUpdateCondQuadForm} is $o(d^2)$ because it is a summation
of quadratic terms.

Summing it up, we derived an upper limit \eqref{PDUpdateCondQuadForm}
for the step size of a block coordinate (row-column) step
(\eqref{gradMtx} and \eqref{updateEq}) that ensures that the update is
PD. The computational complexity for the evaluation of
\eqref{PDUpdateCondQuadForm} is $o(d^2)$. Nevertheless, evaluating
\eqref{PDUpdateCondQuadForm} requires the evaluation of $\W^{-1}$ and
$\mat{A}^{-1}$. The most efficient implementation is to update
$\W^{-1}$ following a block coordinate step, and to derive
$\mat{A}^{-1}$ from $\W^{-1}$ before taking the next step.

\subsubsection{Complete the step update}

Next, we demonstrate how to evaluate $\W^{*-1}$ following a coordinate
step \eqref{updateEq} and how to evaluate $\mat{A}^{*-1}$ before
taking the next coordinate step. Both evaluations will cost a
computational complexity of $o(d^2)$.

Evaluating $\W^{*-1}$ can be easily taken using the Sbury matrix
identity \todo{addref}. By reformulating \eqref{updateEq} and
\eqref{gradMtx} according to the Woodbury matrix identity notation, we
can write
\begin{equation}
  \mat{W^*} = \W+\mat{\widetilde{G}}
  \label{updateEqWDB}
\end{equation}
where
\begin{equation}
  \mat{\widetilde{G}} = \mat{U}\mat{C}\mat{V} = \left[ \begin{matrix}
  \vec{u} & \vec{e_k} \end{matrix} \right] \left[ \begin{matrix} \eta
  & 0 \\ 0 & \eta \end{matrix} \right] \left[ \begin{matrix}
  \vec{e_k}\T \\ \vec{u}\T \end{matrix} \right]
  \label{gradMtxWDB}
\end{equation}

Utilizing the Woodbury matrix identity result with
\begin{equation}
\begin{array}{lcl}
\W^{*-1} = \\
\W^{-1} - \W^{-1} \mat{U} (\eta^{-1} I_{2x2} + \mat{V} \W^{-1} \mat{U})^{-1} \mat{V} \W^{-1}
\end{array}
\label{InvWwdb}
\end{equation}


Last, we evaluate $\mat{A}^{-1}$ before a coordinate step given $\W$
and $\W^{-1}$, using the Schur complement and its corresponding
notation \eqref{schurNotationPreUpdate}:
\begin{equation}
  \begin{array}{l}
    \W^{-1} =  \\
    \left[ \begin{array}{cc} s & -s \vec{B}\T \mat{A}^{-1} \\ -(s \vec{B}\T \mat{A}^{-1} )\T & (\mat{A}^{-1} + \mat{A}^{-1} \vec{B} s \vec{B}\T \mat{A}^{-1} ) \end{array}  \right]
  \end{array}
  \label{BlockInvW}
\end{equation}
where $s$ is a scalar that denotes the Schur Complement:  $s \eqdef (\mat{C}-\vec{B}\T \mat{A}^{-1} \vec{B})$. Hence
\begin{equation}
  s = \W^{-1}_{(1,1)}
  \label{invWkk}
\end{equation}
\begin{equation}
  -s \vec{B}\T \mat{A}^{-1} = -\W^{-1}_{(1,1)} \vec{B}\T \mat{A}^{-1} = \W^{-1}_{(1,2:)}
\end{equation}
\begin{equation}
  \vec{B}\T \mat{A}^{-1} = -\frac{\W^{-1}_{(1,2:)}}{\W^{-1}_{(1,1)} }
\end{equation}
\begin{equation}
  \mat{A}^{-1}\vec{B} = (\vec{B}\T \mat{A}^{-1})\T
  \label{invA_Btrans}
\end{equation}
Assign above at the lower right block of \eqref{BlockInvW} results with
\begin{equation}
  \mat{A}^{-1} + \frac{1}{\W^{-1}_{(1,1)} } \W^{-1}_{(2:,1)} (\W^{-1}_{(2:,1)})\T = \W^{-1}_{(2:,2:)}
\end{equation}

Finally, $A^{-1}$ is extracted to be:
\begin{equation}
\mat{A}^{-1} = \W^{-1}_{(2:,2:)}- \frac{\W^{-1}_{(2:,1)} \W^{-1}_{(1,2:)}}{\W^{-1}_{(1,1)}}
\label{InvA}
\end{equation}
and the computational complexity of \eqref{InvA} is $o(d^2)$

\todo{Discuss numerical stability ?}

All of the above is summarized in Algorithm \ref{alg:comet}

\begin{algorithm}[tb]
   \caption{COMET}
   \label{alg:comet}
\begin{algorithmic}[1]
   \STATE {\bfseries input:} training data, max number of steps, $\alpha$, $\beta$
   \STATE {\bfseries initialize:} 
   \STATE Generate a triplet set $T$, $\W  \leftarrow I_{d \times d}$ , $\W^{-1}  \leftarrow I_{d \times d}$

   \REPEAT 
   \STATE Choose a coordinate $k \in {1..d}$ randomly
   \STATE Evaluate $\mat{A}^{-1}$ using $\W^{-1}$ \eqref{InvA}
   \STATE Evaluate the coordinate step gradient matrix \eqref{gradMtx}
   \STATE Choose the step size, with an upper limit  \eqref{PDUpdateCondQuadForm}
   \STATE Update the metric to $\newW$ \eqref{updateEq}
   \STATE Update the metric inverse to $\newW^{-1}$ \eqref{InvWwdb}
   \UNTIL{max number of steps}
\end{algorithmic}
\end{algorithm}

\subsection{Computational complexity}
Our algorithm is competitively fast. Each step takes a fixed amount of
operations. It keeps $\W$ within the PD cone and therefore avoid a
costly projection to the PD cone following each step. Here, we
evaluate the computational complexity for a single coordinate
step. The complexity is comprised of an evaluation of the gradient
elements and updating $\W$, $\W^{-1}$ and $\mat{A}^{-1}$.

We evaluate the computational complexity of a single coordinate step \eqref{gradMat}. Each element $\delta_{(i,j)}$ of the gradient matrix \eqref{gradMat} equals
\begin{multline}
\delta_{(i,j)} = \sum\limits_{t\in T}{ [\tfrac{1}{2}[(\q_{t})_i(\Delta\p_{t}\T)_j + (\Delta\p_{t}\T)_i(\q_{t})_j\T] } \cdot \textbf{1}(\lambda_t)  \\ 
 - \alpha \cdot \W^{-1}_{(i,j)} + \beta \cdot \W_{(i,j)}
\label{gradMatElem}
\end{multline}

If the training data is dense, evaluating the sum in
\eqref{gradMatElem} costs $o(|T|)$ operations, where $|T|$ is the
number of triplets. Nonetheless, if the training data is sparse, with
a sparsity coefficient $\gamma$, $ 0< \gamma <1 $, then evaluating the
sum in \eqref{gradMatElem} will cost an average of $o(\gamma^2 |T|)$
operations, because we can accumulate only the elements that are both
non-zeros in $(\q_{t})_i$ and in $(\Delta\p_{t}\T)_j $ (same
goes to $(\q_{t})_j$ and $(\Delta\p_{t}\T)_i$). Evaluating
$\W^{-1}_{(i,j)}$ and $\W_{(i,j)}$ costs $o(1)$ since we always keep
the updated versions of $\W$ and $\W^{-1}$. Evaluating all the
gradient elements $\delta_{(k,:)}$ in a single row $k$ then costs
$o(d\cdot \gamma^2 |T|)$. Accounting for $o(d^2)$ which is the cost of
evaluating each of \eqref{InvA}, \eqref{PDUpdateCondQuadForm} and
\eqref{InvWwdb}, sums up for a computational complexity cost of
$o(\gamma^2 d |T| + d^2)$ per coordinate step.

Finally, the total computational complexity cost of our approach is
$o(N \cdot (\gamma d)^2 |T| + N \cdot d^3)$ while taking $N \dot d$
coordinate steps, where N is the number of iterations over all the
coordinates. Our empirical experimental results showed that our
approach converges for choosing $N$ between 5 to 10.

With regard to memory usage, holding the triplets costs $o(\gamma d
|T|)$ elements, holding $\W$ and $\W^{-1}$ costs $o(d^2)$. The total
memory usage is $o(\gamma d |T| + d^2)$

\section{Convergence}
Our method is based on minimizing a strongly convex function using
block-coordinate descent. There is a well established body of work
showing that when using non-overlapping blocks, block-coordinate
descent iterates converge w.h.p. in a linear rate to the optimum value
\cite{nesterov2012efficiency,richtarik2014iteration}.  However, the
blocks we use in our method are overlapping - for example the $(1,2)$
coordinate of the matrix is a part of both the 1\textsuperscript{st}
and the 2\textsuperscript{nd} column-row. We thus make use of a more
general convergence result applicable to overlapping blocks, given by
\citet{richtarik2013optimal}.  In their paper,
\citet{richtarik2013optimal} give sufficient conditions for linear
convergence of overlapping block-coordinate descent for a strongly
convex smooth objective.  In order for these conditions to hold, we
must slightly modify the objective function $L({\W})$. The objective
$L(\W)$ is strongly convex but not smooth, since the gradient of the
$\log \det$ term is unbounded near the envelope of the positive
definite cone. Let $\tilde{L}({\W}) = L({\W + \kappa I_d})$, where
$I_d$ is the $d \times d$ identity matrix, and $\kappa$ is a fixed
parameter.  Note that our algorithm can easily minimize $\tilde{L}$,
the only difference being that we now need to maintain both $\W^{-1}$
and $(\W+\kappa I_d)^{-1}$, which does not change the asymptotic
computational complexity. The additional $\kappa I_d$ term acts as a
prior, where we add a Euclidean distance term to the distance we
learn.  We now show that the modified objective $\tilde{L}$ obeys
Assumption $4$ and Assumption $5$ of
\citet{richtarik2013optimal}.Thereby, according to \citet[Theorems 3
and 7]{richtarik2013optimal}, Algorithm \ref{alg:comet} converges to
the optimum value in a linear rate.

\begin{theorem}
  Let $\tL=\sum\limits_{t\in T}{l_{\W + \kappa I}(\q_t, \p_{t}^{+},
    \p_{t}^{-})} - \alpha \cdot \log \det(\W + \kappa I) +
    \tfrac{1}{2} \beta \cdot \| \W + \kappa I \|_{F}^{2}$, where
    $l_{\W + \kappa I}$ is either the squared hinge loss or the
    log-loss, and $\tilde{L}$ is defined over the positive
    semidefininte cone.  Then $\tL$ is strongly convex, and the
    following two conditions hold:
    \begin{enumerate}
    \item Assumption 4, \citet{richtarik2013optimal}: $\tilde{L}$ has
      Lipschitz gradient with respect to the matrix coordinates: There
      exist positive constants $L_{ij}$ $1 \leq i \leq d, 1 \leq j
      \leq d$, such that $\| \frac{\partial \tL}{W_{ij}} -
      \frac{\partial \tilde{L}(\W + t \E_{ij})}{W_{ij}} \|_F \leq
      L_{ij} |t|$ for all $t \in \mathbb{R}$ such that $\W + t\E_{ij}$
      is positive semidefinite, where $\E_{ij}$ is a symmetric matrix
      with all zeros except the $(i,j)$ and $(j,i)$ coordinates.
    \item Assumption 5, \citet{richtarik2013optimal}: $\tL = \sum_{S
      \in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite
      collection of subsets of matrix index pairs (for example a
      row-column), and $f_S$ are differentiable convex functions such
      that $f_S$ depends only on coordinates $(i,j) \in S$.
    \end{enumerate}
\end{theorem}

\begin{proof}
  
  \begin{enumerate}
    The objective $\tL$ is comprised of three terms - (1) the sum of
    loss terms, (2) the $\log \det$ term, and (3) the Frobenius
    regularization term. The Frobenius norm term ensures that $\tL$ is
    at least $\beta$ strongly-convex.
    \item Assumption 4, \citet{richtarik2013optimal}: Straightforward
        computation shows that the terms (1) and (3) have Lipschitz
        gradients: The Lipschitz constant $L_{ij}$ for the gradient of
        the sum of loss terms is bounded by $\sum_{t \in T}
        (\vec{{q_t}_i} {\Delta \vec{p_t}}_j)^2 + (\vec{{q_t}_j}
        {\Delta \vec{p_t}}_i)^2$. The Lipschitz constant for the
        Frobenius regularization is $\beta$.  We now show the the
        $\log \det$ term also has Lipschitz directional derivatives,
        given that $\kappa >0$:
	\begin{align}
	  &| \frac{\partial \log \det (\W + \kappa I)}{\W_{ij}} -
	  \frac{\partial \log \det (\W + \kappa I + t \E_{ij})}{\W_{ij}}| = \\
	  &| (\W + \kappa I)^{-1} - (\W + \kappa I + t\E_{ij})^{-1} | = \\
	\end{align}
	
    \item Assumption 5, \citet{richtarik2013optimal}: $\tL = \sum_{S
	\in \mathcal{S}} f_S (\W)$, where $\mathcal{S}$ is a finite
	collection of subsets of matrix index pairs (for example a
	row-column), and $f_S$ are differentiable convex functions
	such that $f_S$ depends only on coordinates $(i,j) \in S$.
    \end{enumerate}
\end{proof}

\begin{corollary}
  Let $\W^k$ be the $k$ iterate of Algorithm \ref{alg:comet} with
  objective function $\tilde{L}$, and let $\tilde{L}^*$ be the optimal
  value of $\tilde{L}$ on the PSD cone.  If $t >$ \todo{compute what
    should be here}, then $Prob(\tilde{L}(\W^k) - \tilde{L}^* \leq
  \epsilon) \geq 1-\rho$.
\end{corollary}
\begin{proof}
  proof here
\end{proof}
\section{Experiments}
%  Note that we've also tested our approach with Frobenius norm regularization, but haven't seen a significant improvement for utilizing it.
% number of steps < 10*d

\subsection{Experimental setup}
\subsection{Competing methods}
\subsection{Retrieval of similar images}
\subsection{Document retrieval}


\section{Summary}
This is the best paper ever.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliography{comet}
\bibliographystyle{icml2015}







\appendix{Derivation of the gradient} 

We evaluate a matrix gradient step $\frac{\partial {l_t
(\W)}}{\partial \W}$ of an arbitrary triplet $t$. We define the linear
part of the hinge loss of a triplet $t$ by
\begin{equation}
  \lambda_t \eqdef \lambda_t(\W, \q_t, \p_{t}^{+}, \p_{t}^{-}) \eqdef 1-\q_{t}\T \W \p_{t}^{+}+\q_{t}\T\W\p_{t}^{-}
\end{equation}

We note that $\W$ is PD and therefore symmetric, therefore we will
require its gradient to be symmetric by replacing $\W$ with
$\tfrac{1}{2}(\W + \W\T)$.

The derivative of the ranking loss is then given by
\begin{equation}
  \frac{\partial {l_{\W}^{t}}}{\partial \W} =
  \tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T]\cdot
  {l'}(\lambda_t)
  \label{dlossranking}
\end{equation} 
where $l'(x) \eqdef \frac{\partial {l(x)}}{\partial x}$, $\Delta\p_{t}
\eqdef (\p_{t}^{-} - \p_{t}^{+})$

Replacing the linear hinge loss \eqref{hingelt} in
\eqref{dlossranking}, we observe that it is not differentiable in
$\lambda_t = 0$, but it has a sub gradient matrix

\begin{equation}
  \frac{\partial {l_t (\W)}}{\partial \W} =
  \tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T] \cdot
  \textbf{1}(\lambda_t)
\end{equation}
where $\textbf{1}(x)$ is an indicator for $x>0$. Hence it is non-zero
when \eqref{hingelt} is positive or zero otherwise. Note that
replacing the linear hinge loss by a logarithmic loss would have yield
a continues $\operatorname{sigmoid}(\lambda_t)$ instead of
$\textbf{1}(\lambda_t)$.  We emphasis that $\frac{\partial {l_t
(\W)}}{\partial \W}$ is a matrix, which represents the gradient of
$l_{\W}^{t}$ with respect to each the elements of $W$.

The matrix gradient of $\tfrac{1}{2} \| \W \|_{F}^{2}$ equals $\W$ and
the matrix gradient of $\log \det(\W)$ equals $\W^{-1}$. Therefore,
the matrix gradient of the objective function $L(\W)$ can be easily
determined:
\begin{multline}
  \frac{\partial {L (\W)}}{\partial \W} = \sum\limits_{t\in T}{\{
  [\tfrac{1}{2}[\q_{t}\Delta\p_{t}\T + \Delta\p_{t}\q_{t}\T \} } \cdot
  \textbf{1}(\lambda_t)] \\- \alpha \cdot \W^{-1} + \beta \cdot \W
  \label{gradMat}
\end{multline}

Taking the gradient of the objective which is the sum of of hinge
losses over all the triplets in a batch is a straight-forward gradient
descent approach and has already been utilized by \cite{qian}. Its
drawback is that it can't ensure that the update following the
gradient step will keep $\W$ as positive definite. On \cite{qian} a
projection to the PSD cone is applied following each update.





\end{document} 

